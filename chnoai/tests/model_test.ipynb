{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads chess move image which contains multiple symbols\n",
    "chmove_image = cv2.imread('images\\\\sheets\\\\notex3_b1.png')\n",
    "\n",
    "# Converts RGB to Gray scale\n",
    "chmove_image_gray = cv2.cvtColor(chmove_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Uses Otsu method for thresholding the image \n",
    "ret, chmove_image_thresh = cv2.threshold(chmove_image_gray, 0, 255, cv2.THRESH_OTSU)\n",
    "\n",
    "ctrs, _ = cv2.findContours(chmove_image_thresh.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "sorted_ctrs = sorted(ctrs, key=lambda ctr: cv2.boundingRect(ctr)[0])\n",
    "\n",
    "# Calculate average bounding contour area\n",
    "areas = [cv2.contourArea(ctr) for ctr in ctrs]\n",
    "average_area = sum(areas) / len(areas)\n",
    "\n",
    "# Define thresholds as a percentage of the average area\n",
    "lower_threshold = 625  # 0.5 * average_area\n",
    "upper_threshold = 6400 # 6.0 * average_area\n",
    "\n",
    "extracted_symbols = []\n",
    "for i, ctr in enumerate(sorted_ctrs):\n",
    "    x, y, w, h = cv2.boundingRect(ctr)\n",
    "    area = w*h\n",
    "    if lower_threshold < area < upper_threshold:\n",
    "        # rect = cv2.rectangle(chmove_image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        # cv2.imshow('rect', rect)\n",
    "        extracted_symbols.append(chmove_image_thresh[y:y + h, x:x + w])\n",
    "\n",
    "# cv2.imshow('chmove_image', chmove_image)\n",
    "# cv2.imshow('chmove_image_thresh', chmove_image_thresh)\n",
    "# cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = tf.keras.models.load_model(\"models\\channo_v0.4.keras\")\n",
    "\n",
    "# Define result list\n",
    "result = []\n",
    "\n",
    "# Testing\n",
    "filtered_images = []\n",
    "\n",
    "for extracted_symbol in extracted_symbols:\n",
    "    # Define the desired size of the square image (AxA)\n",
    "    desired_size = max(extracted_symbol.shape) + 10  # offset\n",
    "\n",
    "    # Create a blank square image of the desired size\n",
    "    resized_image = np.ones((desired_size, desired_size), dtype=np.uint8) * 255\n",
    "\n",
    "    # Calculate the position to place the original image in the center\n",
    "    x_offset = (desired_size - extracted_symbol.shape[1]) // 2\n",
    "    y_offset = (desired_size - extracted_symbol.shape[0]) // 2\n",
    "\n",
    "    # Place the original image in the center of the blank square image\n",
    "    resized_image[y_offset:y_offset + extracted_symbol.shape[0], x_offset:x_offset + extracted_symbol.shape[1]] = extracted_symbol\n",
    "\n",
    "    # Downscale the resized image to the target size (e.g., 28x28)\n",
    "    downscaled_image = cv2.resize(resized_image, (28, 28))\n",
    "\n",
    "    # Invert, normalize and reshape image to give input our model\n",
    "    filtered_image = 255 - downscaled_image             # Invert\n",
    "\n",
    "    # Testing\n",
    "    # filtered_images.append(cv2.GaussianBlur(filtered_image, (5, 5), 0))\n",
    "    filtered_images.append(filtered_image)\n",
    "    \n",
    "    filtered_image = filtered_image / 255.0             # Normalize\n",
    "    filtered_image = filtered_image.reshape(1, 28, 28)  # Reshape\n",
    "\n",
    "    # Pass filtered image to our model\n",
    "    predictions = model.predict(filtered_image)\n",
    "    predicted_class = np.argmax(predictions[0])\n",
    "    print(predictions)\n",
    "    result.append(predicted_class)\n",
    "\n",
    "# Plotting all the images\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(filtered_images), figsize=(7, 7))\n",
    "for ax, img in zip(axes, filtered_images):\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_symbols(chmove_image):\n",
    "    # Converts RGB to Gray scale\n",
    "    chmove_image_gray = cv2.cvtColor(chmove_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Uses Otsu method for thresholding the image \n",
    "    ret, chmove_image_thresh = cv2.threshold(chmove_image_gray, 0, 255, cv2.THRESH_OTSU)\n",
    "\n",
    "    ctrs, _ = cv2.findContours(chmove_image_thresh.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    sorted_ctrs = sorted(ctrs, key=lambda ctr: cv2.boundingRect(ctr)[0])\n",
    "\n",
    "    # Calculate average bounding contour area\n",
    "    areas = [cv2.contourArea(ctr) for ctr in ctrs]\n",
    "    average_area = sum(areas) / len(areas)\n",
    "\n",
    "    # Define thresholds as a percentage of the average area\n",
    "    lower_threshold = 625  # 0.5 * average_area\n",
    "    upper_threshold = 6400 # 6.0 * average_area\n",
    "\n",
    "    extracted_symbols = []\n",
    "    for i, ctr in enumerate(sorted_ctrs):\n",
    "        x, y, w, h = cv2.boundingRect(ctr)\n",
    "        area = w*h\n",
    "        if lower_threshold < area < upper_threshold:\n",
    "            extracted_symbols.append(chmove_image_thresh[y:y + h, x:x + w])\n",
    "\n",
    "    # Load model\n",
    "    model = tf.keras.models.load_model(\"models\\channo_v0.4.keras\")\n",
    "\n",
    "    # Define result list\n",
    "    result = []\n",
    "\n",
    "    for extracted_symbol in extracted_symbols:\n",
    "        # Define the desired size of the square image (AxA)\n",
    "        desired_size = max(extracted_symbol.shape) + 10  # offset\n",
    "\n",
    "        # Create a blank square image of the desired size\n",
    "        resized_image = np.ones((desired_size, desired_size), dtype=np.uint8) * 255\n",
    "\n",
    "        # Calculate the position to place the original image in the center\n",
    "        x_offset = (desired_size - extracted_symbol.shape[1]) // 2\n",
    "        y_offset = (desired_size - extracted_symbol.shape[0]) // 2\n",
    "\n",
    "        # Place the original image in the center of the blank square image\n",
    "        resized_image[y_offset:y_offset + extracted_symbol.shape[0], x_offset:x_offset + extracted_symbol.shape[1]] = extracted_symbol\n",
    "\n",
    "        # Downscale the resized image to the target size (e.g., 28x28)\n",
    "        downscaled_image = cv2.resize(resized_image, (28, 28))\n",
    "\n",
    "        # Invert, normalize and reshape image to give input our model\n",
    "        filtered_image = 255 - downscaled_image             # Invert\n",
    "        filtered_image = filtered_image / 255.0             # Normalize\n",
    "        filtered_image = filtered_image.reshape(1, 28, 28)  # Reshape\n",
    "\n",
    "        # Pass filtered image to our model\n",
    "        predictions = model.predict(filtered_image)\n",
    "        predicted_class = np.argmax(predictions[0])\n",
    "        result.append(predicted_class)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chmove_images = []\n",
    "chmove_expection_list = []\n",
    "chmove_prediction_list = []\n",
    "\n",
    "for i in range(5):\n",
    "    chmove_image_w = cv2.imread(f'images\\\\sheets\\\\notex3_w{(i + 1)}.png')\n",
    "    chmove_image_b = cv2.imread(f'images\\\\sheets\\\\notex3_b{(i + 1)}.png')\n",
    "    chmove_image_w[chmove_image_w > 200] = 180\n",
    "    chmove_image_b[chmove_image_b > 200] = 180\n",
    "    chmove_images.append(chmove_image_w)\n",
    "    chmove_images.append(chmove_image_b)\n",
    "\n",
    "chmove_expection_list.append([13, 4]);          chmove_expection_list.append([13, 5])\n",
    "chmove_expection_list.append([21, 14, 3]);      chmove_expection_list.append([21, 11, 6])\n",
    "chmove_expection_list.append([20, 10, 5]);      chmove_expection_list.append([9, 6])\n",
    "chmove_expection_list.append([20, 9, 4]);       chmove_expection_list.append([21, 14, 6])\n",
    "chmove_expection_list.append([12, 3]);          chmove_expection_list.append([20, 13, 7])\n",
    "# chmove_expection_list.append([20, 23, 11, 6]);  chmove_expection_list.append([10, 23, 11, 6])\n",
    "# chmove_expection_list.append([21, 23, 13, 5]);  chmove_expection_list.append([20, 10, 7])\n",
    "# chmove_expection_list.append([0, 22, 0]);       chmove_expection_list.append([16, 6])\n",
    "# chmove_expection_list.append([21, 11, 3]);      chmove_expection_list.append([0, 22, 0])\n",
    "# chmove_expection_list.append([20, 13, 3]);      chmove_expection_list.append([12, 5])\n",
    "\n",
    "for chmove_image in chmove_images:\n",
    "    prediction = predict_symbols(chmove_image)\n",
    "    chmove_prediction_list.append(prediction)\n",
    "\n",
    "total_sample = len(chmove_expection_list)\n",
    "total_correct_prediction = 0\n",
    "for i in range(len(chmove_expection_list)):\n",
    "    if len(chmove_expection_list[i]) == len(chmove_prediction_list[i]):\n",
    "        if chmove_expection_list[i] == chmove_prediction_list[i]:\n",
    "            total_correct_prediction += 1\n",
    "\n",
    "print(f'Accuracy: {(total_correct_prediction / total_sample)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocrenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
