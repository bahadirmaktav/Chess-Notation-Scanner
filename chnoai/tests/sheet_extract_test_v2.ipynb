{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple(images, rc):\n",
    "    if rc == 'row':\n",
    "        fig, axes = plt.subplots(nrows=len(images), ncols=1, figsize=(14, 14))\n",
    "        for ax, img in zip(axes, images):\n",
    "            ax.imshow(img, cmap='gray')\n",
    "            ax.axis('off')\n",
    "        plt.show()\n",
    "    elif rc == 'column':\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=len(images), figsize=(14, 14))\n",
    "        for ax, img in zip(axes, images):\n",
    "            ax.imshow(img, cmap='gray')\n",
    "            ax.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"rc is not valid!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_vertical(image):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Uses Otsu method for thresholding the image \n",
    "    ret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Detect vertical lines\n",
    "    vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 80))\n",
    "    detect_vertical = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, vertical_kernel, iterations=2)\n",
    "    cnts = cv2.findContours(detect_vertical, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]   # Why?\n",
    "\n",
    "    cnts_x_averages = []\n",
    "    for cnt in cnts:\n",
    "        temp_total = 0\n",
    "        for pnt in cnt:\n",
    "            temp_total += pnt[0][0]\n",
    "        cnts_x_averages.append(temp_total / len(cnt))\n",
    "    cnts_x_averages.sort()\n",
    "    cnts_x_averages_differences = [abs(cnts_x_averages[i] - cnts_x_averages[i + 1]) for i in range(len(cnts_x_averages) - 1)]\n",
    "    cnts_x_averages_differences.sort()\n",
    "    average_difference_offset = int(sum(cnts_x_averages_differences[-4:]) / len(cnts_x_averages_differences[-4:])) - 50 # offset\n",
    "\n",
    "    sliced_images = []\n",
    "    for i in range(1, len(cnts_x_averages)):\n",
    "        if cnts_x_averages[i] - cnts_x_averages[i - 1] > average_difference_offset:\n",
    "            temp_sliced_image = image[ : len(image[0]), int(cnts_x_averages[i - 1]) : int(cnts_x_averages[i])]\n",
    "            sliced_images.append(temp_sliced_image)\n",
    "\n",
    "    return sliced_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_horizontal(image):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Uses Otsu method for thresholding the image \n",
    "    ret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Detect horizontal lines\n",
    "    horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (80,1))\n",
    "    detect_horizontal = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, horizontal_kernel, iterations=2)\n",
    "    cnts = cv2.findContours(detect_horizontal, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]   # Why?\n",
    "\n",
    "    cnts_y_averages = []\n",
    "    for cnt in cnts:\n",
    "        temp_total = 0\n",
    "        for pnt in cnt:\n",
    "            temp_total += pnt[0][1]\n",
    "        cnts_y_averages.append(temp_total / len(cnt))\n",
    "    cnts_y_averages.sort()\n",
    "\n",
    "    filtered_cnts_y_averages = [cnts_y_averages[0]]\n",
    "    for i in range(1, len(cnts_y_averages)):\n",
    "        if cnts_y_averages[i] > cnts_y_averages[i - 1] + 20: # offset\n",
    "            filtered_cnts_y_averages.append(cnts_y_averages[i])\n",
    "\n",
    "    sliced_images = []\n",
    "    for i in range(1, len(filtered_cnts_y_averages)):\n",
    "        temp_sliced_image = image[int(filtered_cnts_y_averages[i - 1]) : int(filtered_cnts_y_averages[i]), : len(image[0])]\n",
    "        sliced_images.append(temp_sliced_image)\n",
    "\n",
    "    return sliced_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_move(image):\n",
    "    # ##\n",
    "    width_cut_offset = 15\n",
    "    height_cut_offset = 10\n",
    "    cut_image = image[height_cut_offset : len(image) - height_cut_offset, width_cut_offset : len(image[0]) - width_cut_offset]\n",
    "\n",
    "    # ##\n",
    "    gray = cv2.cvtColor(cut_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Uses Otsu method for thresholding the image \n",
    "    ret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU)\n",
    "\n",
    "    # Add white border to improve contour find algorithm\n",
    "    thresh = cv2.copyMakeBorder(thresh, height_cut_offset, height_cut_offset, width_cut_offset, width_cut_offset, cv2.BORDER_CONSTANT, value=(255, 255, 255))\n",
    "\n",
    "    # Create an empty white image with the same size as the original image\n",
    "    # thresh[ : width_border_size, : len(thresh[0])] = 255\n",
    "    # thresh[ : len(thresh), len(thresh[0]) - height_border_size : len(thresh[0])] = 255\n",
    "    # thresh[len(thresh) - width_border_size : len(thresh), : len(thresh[0])] = 255\n",
    "    # thresh[ : len(thresh), : height_border_size] = 255\n",
    "\n",
    "    ctrs, _ = cv2.findContours(thresh.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    sorted_ctrs = sorted(ctrs, key=lambda ctr: cv2.boundingRect(ctr)[0])\n",
    "\n",
    "    # Calculate average bounding contour area\n",
    "    areas = [cv2.contourArea(ctr) for ctr in ctrs]\n",
    "    average_area = sum(areas) / len(areas)\n",
    "\n",
    "    # Define thresholds as a percentage of the average area\n",
    "    lower_threshold = 500  # 0.5 * average_area\n",
    "    upper_threshold = 6400 # 6.0 * average_area\n",
    "\n",
    "    extracted_symbols = []\n",
    "    for i, ctr in enumerate(sorted_ctrs):\n",
    "        x, y, w, h = cv2.boundingRect(ctr)\n",
    "        area = w*h\n",
    "        if lower_threshold < area < upper_threshold:\n",
    "            extracted_symbols.append(thresh[y:y + h, x:x + w])\n",
    "            # TEST\n",
    "            # rect = cv2.rectangle(thresh, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            # cv2.imshow('rect', rect)\n",
    "\n",
    "    # Load model\n",
    "    model = tf.keras.models.load_model(\"..\\\\models\\\\channo_v0.4.keras\")\n",
    "\n",
    "    # Define result list\n",
    "    result = []\n",
    "\n",
    "    # TEST\n",
    "    filtered_images = []\n",
    "\n",
    "    for extracted_symbol in extracted_symbols:\n",
    "        # Define the desired size of the square image (AxA)\n",
    "        desired_size = max(extracted_symbol.shape) + 15  # offset\n",
    "\n",
    "        # Create a blank square image of the desired size\n",
    "        resized_image = np.ones((desired_size, desired_size), dtype=np.uint8) * 255\n",
    "\n",
    "        # Calculate the position to place the original image in the center\n",
    "        x_offset = (desired_size - extracted_symbol.shape[1]) // 2\n",
    "        y_offset = (desired_size - extracted_symbol.shape[0]) // 2\n",
    "\n",
    "        # Place the original image in the center of the blank square image\n",
    "        resized_image[y_offset:y_offset + extracted_symbol.shape[0], x_offset:x_offset + extracted_symbol.shape[1]] = extracted_symbol\n",
    "\n",
    "        # Downscale the resized image to the target size (e.g., 28x28)\n",
    "        downscaled_image = cv2.resize(resized_image, (28, 28))\n",
    "\n",
    "        # Invert, normalize and reshape image to give input our model\n",
    "        filtered_image = 255 - downscaled_image             # Invert\n",
    "\n",
    "        # TEST\n",
    "        # filtered_images.append(cv2.GaussianBlur(filtered_image, (5, 5), 0))\n",
    "        filtered_images.append(filtered_image)\n",
    "\n",
    "        filtered_image = filtered_image / 255.0             # Normalize\n",
    "        filtered_image = filtered_image.reshape(1, 28, 28)  # Reshape\n",
    "\n",
    "        # Pass filtered image to our model\n",
    "        predictions = model.predict(filtered_image)\n",
    "        predicted_class = np.argmax(predictions[0])\n",
    "        result.append(predicted_class)\n",
    "\n",
    "    # TEST\n",
    "    # print(len(filtered_images))\n",
    "    # plot_multiple(filtered_images, 'row')\n",
    "\n",
    "    # TEST\n",
    "    # Display the original image with detected lines\n",
    "    # cv2.imshow('Detected lines', image)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_move_as_string(values):\n",
    "    custom_mapping = {\n",
    "        0:  '0',\n",
    "        1:  '1',\n",
    "        2:  '2',\n",
    "        3:  '3',\n",
    "        4:  '4',\n",
    "        5:  '5',\n",
    "        6:  '6',\n",
    "        7:  '7',\n",
    "        8:  '8',\n",
    "        9:  'a',\n",
    "        10: 'b',\n",
    "        11: 'c',\n",
    "        12: 'd',\n",
    "        13: 'e',\n",
    "        14: 'f',\n",
    "        15: 'g',\n",
    "        16: 'h',\n",
    "        17: 'K',\n",
    "        18: 'Q',\n",
    "        19: 'R',\n",
    "        20: 'B',\n",
    "        21: 'N',\n",
    "        22: '-',\n",
    "        23: 'x',\n",
    "        24: '+',\n",
    "        25: '#'\n",
    "    }\n",
    "    result = ''\n",
    "    for value in values:\n",
    "        result += custom_mapping.get(value)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the image\n",
    "image = cv2.imread('images\\\\sheets\\\\example_sheet.png')\n",
    "\n",
    "# Converts bgr to rgb color scale, default reads as bgr why?\n",
    "# image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Result matrix\n",
    "player_moves = [[], []]\n",
    "\n",
    "vertical_sliced_images = slice_vertical(image)\n",
    "for i, vertical_sliced_image in enumerate(vertical_sliced_images):\n",
    "    horizontal_sliced_images = slice_horizontal(vertical_sliced_image)\n",
    "    # plot_multiple(horizontal_sliced_images, 'column')\n",
    "    for horizontal_sliced_image in horizontal_sliced_images:\n",
    "        move_as_int_list = predict_move(horizontal_sliced_image)\n",
    "        player_moves[i % 2].append(map_to_move_as_string(move_as_int_list))\n",
    "    # move_as_int_list = predict_move(horizontal_sliced_images[6])\n",
    "    # player_moves[i % 2].append(move_as_int_list)\n",
    "    # break\n",
    "\n",
    "print('\\nWhite Moves')\n",
    "print(player_moves[0])\n",
    "\n",
    "print('\\nBlack Moves')\n",
    "print(player_moves[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocrenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
